{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore this, the zip file is gone\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"archive.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset, this is what worked for me due to my directory \n",
    "#train_dataset = GestureSequenceDataset(root_dir=\"./data/data/train\", transform=transform)\n",
    "#test_dataset = GestureSequenceDataset(root_dir=\"./data/data/val\", transform=transform)\n",
    "\n",
    "train_dataset = ImageFolder(root=\"./data/images/train\", transform=transform)\n",
    "test_dataset = ImageFolder(root=\"./data/images/validation\", transform=transform)\n",
    "\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GestureRecognitionModel, self).__init__()\n",
    "\n",
    "        # TODO 2: construct your own model. CNNs+LSTM is recommended\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "    \n",
    "\n",
    "        \n",
    "        self.feature_dim = 128 * 16 * 16 \n",
    "        self.fc = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "        x = self.conv1(x)  # [B, 32, 64, 64]\n",
    "        x = self.conv2(x)  # [B, 64, 32, 32]\n",
    "        x = self.conv3(x)  # [B, 128, 16, 16]\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = GestureRecognitionModel(num_classes=3).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # change the learning rate as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/51], Loss: 1.1730, Accuracy: 45.94%\n",
      "Epoch [2/51], Loss: 1.0549, Accuracy: 51.31%\n",
      "Epoch [3/51], Loss: 1.0035, Accuracy: 53.78%\n",
      "Epoch [4/51], Loss: 0.9880, Accuracy: 54.87%\n",
      "Epoch [5/51], Loss: 0.9762, Accuracy: 55.32%\n",
      "Epoch [6/51], Loss: 0.9438, Accuracy: 57.20%\n",
      "Epoch [7/51], Loss: 0.9196, Accuracy: 58.11%\n",
      "Epoch [8/51], Loss: 0.8958, Accuracy: 59.59%\n",
      "Epoch [9/51], Loss: 0.8821, Accuracy: 60.60%\n",
      "Epoch [10/51], Loss: 0.8770, Accuracy: 60.93%\n",
      "Epoch [11/51], Loss: 0.8648, Accuracy: 61.74%\n",
      "Epoch [12/51], Loss: 0.8532, Accuracy: 62.37%\n",
      "Epoch [13/51], Loss: 0.8420, Accuracy: 62.38%\n",
      "Epoch [14/51], Loss: 0.8318, Accuracy: 63.44%\n",
      "Epoch [15/51], Loss: 0.8271, Accuracy: 63.44%\n",
      "Epoch [16/51], Loss: 0.8118, Accuracy: 64.16%\n",
      "Epoch [17/51], Loss: 0.8055, Accuracy: 64.13%\n",
      "Epoch [18/51], Loss: 0.8003, Accuracy: 64.77%\n",
      "Epoch [19/51], Loss: 0.8004, Accuracy: 64.42%\n",
      "Epoch [20/51], Loss: 0.7903, Accuracy: 65.37%\n",
      "Epoch [21/51], Loss: 0.7901, Accuracy: 64.94%\n",
      "Epoch [22/51], Loss: 0.7776, Accuracy: 65.97%\n",
      "Epoch [23/51], Loss: 0.7703, Accuracy: 66.26%\n",
      "Epoch [24/51], Loss: 0.7687, Accuracy: 66.42%\n",
      "Epoch [25/51], Loss: 0.7697, Accuracy: 66.11%\n",
      "Epoch [26/51], Loss: 0.7606, Accuracy: 66.75%\n",
      "Epoch [27/51], Loss: 0.7557, Accuracy: 67.32%\n",
      "Epoch [28/51], Loss: 0.7502, Accuracy: 67.41%\n",
      "Epoch [29/51], Loss: 0.7543, Accuracy: 67.46%\n",
      "Epoch [30/51], Loss: 0.7445, Accuracy: 67.32%\n",
      "Epoch [31/51], Loss: 0.7419, Accuracy: 67.92%\n",
      "Epoch [32/51], Loss: 0.7471, Accuracy: 67.74%\n",
      "Epoch [33/51], Loss: 0.7370, Accuracy: 68.20%\n",
      "Epoch [34/51], Loss: 0.7278, Accuracy: 68.67%\n",
      "Epoch [35/51], Loss: 0.7340, Accuracy: 68.34%\n",
      "Epoch [36/51], Loss: 0.7341, Accuracy: 68.24%\n",
      "Epoch [37/51], Loss: 0.7258, Accuracy: 68.49%\n",
      "Epoch [38/51], Loss: 0.7303, Accuracy: 68.73%\n",
      "Epoch [39/51], Loss: 0.7097, Accuracy: 69.35%\n",
      "Epoch [40/51], Loss: 0.7135, Accuracy: 68.88%\n",
      "Epoch [41/51], Loss: 0.7022, Accuracy: 69.27%\n",
      "Epoch [42/51], Loss: 0.7021, Accuracy: 69.81%\n",
      "Epoch [43/51], Loss: 0.7023, Accuracy: 69.53%\n",
      "Epoch [44/51], Loss: 0.6987, Accuracy: 69.98%\n",
      "Epoch [45/51], Loss: 0.7134, Accuracy: 69.14%\n",
      "Epoch [46/51], Loss: 0.6892, Accuracy: 70.72%\n",
      "Epoch [47/51], Loss: 0.7010, Accuracy: 69.88%\n",
      "Epoch [48/51], Loss: 0.6915, Accuracy: 70.23%\n",
      "Epoch [49/51], Loss: 0.6899, Accuracy: 70.52%\n",
      "Epoch [50/51], Loss: 0.6868, Accuracy: 70.55%\n",
      "Epoch [51/51], Loss: 0.6889, Accuracy: 70.74%\n"
     ]
    }
   ],
   "source": [
    "num_epochs =  51# set the number of epochs for training\n",
    "'''\n",
    "for images, labels in train_loader:\n",
    "    print(\"Sample input shape:\", images.shape)  # Should be [64, 1, 128, 128]\n",
    "    outputs = model(images)\n",
    "    print(\"Sample output shape:\", outputs.shape)  # Should be [64, num_classes]\n",
    "    break\n",
    "'''\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Loop over each batch\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # it is recommand to save the model regulary during training if you don't have a lof of computational resource\n",
    "\n",
    "    root_path = \"./saved_models\"\n",
    "    model_dir = os.path.join(root_path, \"model_configs\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    if epoch%5 ==0: # change the saving frequency as you want\n",
    "        model_path = os.path.join(root_path, 'model_configs', f'image_model_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GestureRecognitionModel(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=32768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GestureRecognitionModel(num_classes=3)\n",
    "model.load_state_dict(torch.load(\"./saved_models/model_configs/image_model_50.pth\"))  # Load the desired saved model weights, change the path if needed\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.5568607848038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(classification_report(labels_test, predictions, target_names=list(gesture_labels.values())))\\n\\ncon_mat = confusion_matrix(labels_test, predictions, labels=list(gesture_labels.keys()))\\n\\nplt.figure(figsize=(6,6))\\nsns.heatmap(con_mat, annot=True, fmt=\"d\", cmap=\"Reds\", xticklabels=gesture_labels.values(), yticklabels=gesture_labels.values())\\nplt.xlabel(\"Predicted\")\\nplt.ylabel(\"Actual\")\\nplt.title(\"Confusion Matrix\")\\nplt.show()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html for confusion_matrix\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html for classification report \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#gesture_labels = {0: \"Swipe Left\", 1: \"Swipe Right\", 2: \"Stop\", 3: \"Thumbs Up\", 4: \"Thumbs Down\"}\n",
    "predictions = []\n",
    "labels_test = []\n",
    "\n",
    "# TODO 6: evaluate your model on the test dataset\n",
    "\n",
    "#copy pasted and edited from the training and changed to test\n",
    "with torch.no_grad():  # No gradients needed for inference\n",
    "    correct = 0\n",
    "    total=0\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        labels_test.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100*correct/total\n",
    "print(test_accuracy)\n",
    "\n",
    "# TODO 7: Report precision, recall, accuracy, F1 score, and create a confusion matrix on the test set, showing the confusions between labels.\n",
    "\n",
    "'''print(classification_report(labels_test, predictions, target_names=list(gesture_labels.values())))\n",
    "\n",
    "con_mat = confusion_matrix(labels_test, predictions, labels=list(gesture_labels.keys()))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(con_mat, annot=True, fmt=\"d\", cmap=\"Reds\", xticklabels=gesture_labels.values(), yticklabels=gesture_labels.values())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
